x-logging: &id001
  driver: json-file
  options:
    max-size: 50m
    max-file: '3'
x-network-tweaks: &network_tweaks
  dns:
    - 1.1.1.1
    - 1.0.0.1
  dns_opt:
    - use-vc
  sysctls:
    - net.ipv6.conf.all.disable_ipv6=1
    - net.ipv6.conf.default.disable_ipv6=1
volumes:
  portainer_data:
    driver: local
services:
  samba:
    <<: *network_tweaks
    image: ghcr.io/servercontainers/samba:smbd-only-latest
    container_name: samba
    logging: *id001
    hostname: media-server
    ports:
      - 139:139
      - 445:445
    environment:
      - TZ=Etc/UTC
      - ACCOUNT_joe=joe
      - UID_joe=1000
      - SAMBA_VOLUME_CONFIG_downloads=[Downloads]; path = /shares/downloads; browseable = yes; writeable = yes; guest ok = no; valid users = joe; force user = joe; force group = joe
      - SAMBA_VOLUME_CONFIG_media=[Media]; path = /shares/media; browseable = yes; writeable = yes; guest ok = no; valid users = joe; force user = joe; force group = joe
    volumes:
      - ${MEDIA_ROOT:-/srv/usenet/media}:/shares/media:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/shares/downloads:rw,z
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
          - node.labels.storage == true
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test:
        - CMD
        - smbclient
        - -L
        - localhost
        - -U
        - '%'
      interval: 30s
      timeout: 10s
      retries: 3
  nfs-server:
    <<: *network_tweaks
    image: erichough/nfs-server:latest
    logging: *id001
    container_name: nfs-server
    hostname: nfs-server
    ports:
      - 2049:2049
      - 111:111
      - 111:111/udp
      - 32765:32765
      - 32767:32767/udp
    cap_add:
      - SYS_ADMIN
    privileged: true
    volumes:
      - ${MEDIA_ROOT:-/srv/usenet/media}:/media:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}:/config:rw,z
    environment:
      - SHARED_DIRECTORY=/media
      - SHARED_DIRECTORY_2=/downloads
      - SHARED_DIRECTORY_3=/config
      - SYNC=true
      - PERMITTED=192.168.0.0/16
      - NFS_EXPORT_0=/media *(rw,sync,no_subtree_check,no_root_squash)
      - NFS_EXPORT_1=/downloads *(rw,sync,no_subtree_check,no_root_squash)
    restart: unless-stopped
    deploy:
      replicas: 0
  netdata:
    <<: *network_tweaks
    image: netdata/netdata:stable
    container_name: netdata
    hostname: netdata-media-server
    ports:
      - 19999:19999
    cap_add:
      - SYS_PTRACE
    security_opt:
      - apparmor:unconfined
    volumes:
      - /proc:/host/proc:rw
      - /sys:/host/sys:rw
      - /var/run/docker.sock:/var/run/docker.sock:rw
      - /etc/passwd:/host/etc/passwd:rw
      - /etc/group:/host/etc/group:rw
    environment:
      - DOCKER_HOST=/var/run/docker.sock
    restart: unless-stopped
    deploy:
      mode: global
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    logging:
      driver: json-file
      options: &id002
        max-size: 50m
        max-file: '3'
  portainer:
    <<: *network_tweaks
    image: portainer/portainer-ce:latest
    container_name: portainer
    ports:
      - 9000:9000
      - 8000:8000
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:rw
      - portainer_data:/data
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: json-file
      options: *id002
  uptime-kuma:
    <<: *network_tweaks
    image: louislam/uptime-kuma:1
    container_name: uptime-kuma
    environment:
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/uptime-kuma:/app/data:rw,z
    ports:
      - 3001:3001
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: json-file
      options: *id002
  sabnzbd:
    <<: *network_tweaks
    image: docker.io/linuxserver/sabnzbd:latest
    container_name: sabnzbd
    hostname: sabnzbd
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/sabnzbd:/config:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
    ports:
      - "192.168.6.167:8080:8080"  # External IP only - leaves localhost:8080 free for Steam CEF/DeckyLoader
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.performance == high
          - node.labels.storage == true
      resources:
        limits:
          cpus: '2.0'
          memory: 6G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test:
        - CMD
        - curl
        - -f
        - http://localhost:8080
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options: *id002
  transmission:
    <<: *network_tweaks
    image: docker.io/linuxserver/transmission:latest
    container_name: transmission
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/transmission:/config:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}/watch:/watch:rw,z
    ports:
      - 9091:9091
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.performance == high
          - node.labels.storage == true
      resources:
        limits:
          cpus: '1.5'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M
    healthcheck:
      test:
        - CMD
        - curl
        - -f
        - http://localhost:9091/transmission/web/
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options: *id002
  sonarr:
    <<: *network_tweaks
    image: docker.io/linuxserver/sonarr:latest
    container_name: sonarr
    hostname: sonarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/sonarr:/config:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/tv:/tv:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}:/pool:rw,z
    ports:
      - 8989:8989
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '1.0'
          memory: 4G
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      - sabnzbd
      - transmission
    logging:
      driver: json-file
      options: *id002
  radarr:
    <<: *network_tweaks
    image: docker.io/linuxserver/radarr:latest
    container_name: radarr
    hostname: radarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/radarr:/config:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/movies:/movies:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}:/pool:rw,z
    ports:
      - 7878:7878
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '1.0'
          memory: 4G
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      - sabnzbd
      - transmission
    logging:
      driver: json-file
      options: *id002
  bazarr:
    <<: *network_tweaks
    image: docker.io/linuxserver/bazarr:latest
    container_name: bazarr
    hostname: bazarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/bazarr:/config:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/tv:/tv:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/movies:/movies:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}:/pool:rw,z
    ports:
      - 6767:6767
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '0.5'
          memory: 2G
        reservations:
          cpus: '0.1'
          memory: 128M
    depends_on:
      - sonarr
      - radarr
    logging:
      driver: json-file
      options: *id002
  prowlarr:
    <<: *network_tweaks
    image: docker.io/linuxserver/prowlarr:latest
    container_name: prowlarr
    hostname: prowlarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/prowlarr:/config:rw,z
    ports:
      - 9696:9696
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.performance != low
      resources:
        limits:
          cpus: '0.5'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test:
        - CMD
        - curl
        - -f
        - http://localhost:9696/ping
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options: *id002
  whisparr:
    <<: *network_tweaks
    image: docker.io/thespad/whisparr:latest
    container_name: whisparr
    hostname: whisparr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/whisparr:/config:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/movies:/movies:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/tv:/tv:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/adult:/adult:rw,z
    ports:
      - 6969:6969
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: json-file
      options: *id002
  lidarr:
    <<: *network_tweaks
    image: lscr.io/linuxserver/lidarr:latest
    container_name: lidarr
    hostname: lidarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/lidarr:/config:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
      - ${MUSIC_ROOT:-/srv/usenet/media/music}:/music:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}:/pool:rw,z
    ports:
      - 8686:8686
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: json-file
      options: *id002
  readarr:
    <<: *network_tweaks
    image: linuxserver/readarr:develop-0.4.18.2805-ls157
    container_name: readarr
    hostname: readarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/readarr:/config:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
      - ${EBOOKS_ROOT:-/srv/usenet/books/eBooks}:/books:rw,z
      - ${BOOKS_ROOT:-/srv/usenet/books}:/books-all:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}:/pool:rw,z
    ports:
      - 8787:8787
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      - sabnzbd
      - transmission
    logging:
      driver: json-file
      options: *id002
  mylar:
    <<: *network_tweaks
    image: docker.io/linuxserver/mylar3:latest
    container_name: mylar
    hostname: mylar
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/mylar:/config:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
      - ${COMICS_ROOT:-/srv/usenet/books/Comics}:/comics:rw,z
    ports:
      - 8090:8090
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
    logging:
      driver: json-file
      options: *id002
  plex:
    # Using host network for proper remote access (UPnP/NAT-PMP)
    network_mode: host
    image: docker.io/linuxserver/plex:latest
    container_name: plex
    hostname: plex
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - VERSION=docker
      - PLEX_CLAIM=${PLEX_CLAIM:-}
      # AMD GPU VAAPI fix: Container Mesa is too old for RDNA3 (gfx1103/780M)
      # This mod copies Alpine edge Mesa drivers into container for HW transcoding
      # Ref: https://github.com/jefflessard/plex-vaapi-amdgpu-mod
      - DOCKER_MODS=jefflessard/plex-vaapi-amdgpu-mod
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/plex:/config:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}:/media:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/movies:/movies:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/tv:/tv:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/music:/music:rw,z
      - ${BOOKS_ROOT:-/srv/usenet/books}:/books:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}:/pool:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/christmas-movies:/christmas-movies:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/christmas-tv:/christmas-tv:rw,z
    # No ports needed with host network mode
    devices:
      - /dev/dri:/dev/dri
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '2.0'
          memory: 4G
        reservations:
          cpus: '0.25'
          memory: 512M
    logging:
      driver: json-file
      options: *id002
  overseerr:
    <<: *network_tweaks
    image: docker.io/linuxserver/overseerr:latest
    container_name: overseerr
    hostname: overseerr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/overseerr:/config:rw,z
    ports:
      - 5055:5055
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 2G
    logging:
      driver: json-file
      options: *id002
  recyclarr:
    <<: *network_tweaks
    image: ghcr.io/recyclarr/recyclarr:latest
    container_name: recyclarr
    hostname: recyclarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/recyclarr:/config:rw,z
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'
          memory: 256M
    logging:
      driver: json-file
      options: *id002
  # Tdarr Transcoding Stack - AMD VAAPI Hardware Encoding (RDNA3/780M)
  # Enables HEVC transcoding for 40-50% storage savings
  tdarr:
    <<: *network_tweaks
    image: ghcr.io/haveagitgat/tdarr:latest
    container_name: tdarr
    hostname: tdarr
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/Los_Angeles
      - UMASK_SET=002
      - serverIP=0.0.0.0
      - serverPort=8266
      - webUIPort=8265
      - internalNode=true
      - inContainer=true
      - ffmpegVersion=7
      - nodeName=MainNode
      # Worker configuration (declarative - survives fresh deploys)
      # GPU-only transcoding: VAAPI handles encoding efficiently
      - transcodegpuWorkers=${TDARR_TRANSCODE_GPU_WORKERS:-4}
      - transcodecpuWorkers=${TDARR_TRANSCODE_CPU_WORKERS:-0}
      - healthcheckgpuWorkers=${TDARR_HEALTHCHECK_GPU_WORKERS:-1}
      - healthcheckcpuWorkers=${TDARR_HEALTHCHECK_CPU_WORKERS:-0}
      # NOTE: Tdarr Pro license key must be entered via Web UI -> Options
      # License: 2RVB2HW-BW14CDZ-M4J6Z2T-XNB61SY
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/tdarr/server:/app/server:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}/tdarr/configs:/app/configs:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}/tdarr/logs:/app/logs:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/movies:/media/movies:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/tv:/media/tv:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/anime-movies:/media/anime-movies:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/anime-tv:/media/anime-tv:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/christmas-movies:/media/christmas-movies:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/christmas-tv:/media/christmas-tv:rw,z
      - /tmp/tdarr_transcode:/temp:rw,z
    ports:
      - 8265:8265
      - 8266:8266
    devices:
      - /dev/dri:/dev/dri
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        # Resource limits from .env - tune per machine
        limits:
          cpus: '${TDARR_MAIN_CPU_LIMIT:-8.0}'
          memory: ${TDARR_MAIN_MEM_LIMIT:-24G}
        reservations:
          cpus: '${TDARR_MAIN_CPU_RESERVE:-4.0}'
          memory: ${TDARR_MAIN_MEM_RESERVE:-8G}
    logging:
      driver: json-file
      options: *id002
  # Tdarr secondary node for parallel processing
  tdarr-node:
    <<: *network_tweaks
    image: ghcr.io/haveagitgat/tdarr_node:latest
    container_name: tdarr-node
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/Los_Angeles
      - UMASK_SET=002
      - nodeName=SecondaryNode
      - serverIP=tdarr
      - serverPort=8266
      - inContainer=true
      - ffmpegVersion=7
      # Secondary node: GPU-only, fewer workers (from .env)
      - transcodegpuWorkers=${TDARR_NODE_TRANSCODE_GPU_WORKERS:-2}
      - transcodecpuWorkers=${TDARR_NODE_TRANSCODE_CPU_WORKERS:-0}
      - healthcheckgpuWorkers=${TDARR_NODE_HEALTHCHECK_GPU_WORKERS:-1}
      - healthcheckcpuWorkers=${TDARR_NODE_HEALTHCHECK_CPU_WORKERS:-0}
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/tdarr/configs:/app/configs:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}/tdarr/logs:/app/logs:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/movies:/media/movies:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/tv:/media/tv:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/anime-movies:/media/anime-movies:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/anime-tv:/media/anime-tv:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/christmas-movies:/media/christmas-movies:rw,z
      - ${POOL_ROOT:-/var/mnt/pool}/christmas-tv:/media/christmas-tv:rw,z
      - /tmp/tdarr_transcode:/temp:rw,z
    devices:
      - /dev/dri:/dev/dri
    restart: unless-stopped
    deploy:
      resources:
        # Secondary node resources from .env
        limits:
          cpus: '${TDARR_NODE_CPU_LIMIT:-6.0}'
          memory: ${TDARR_NODE_MEM_LIMIT:-12G}
    depends_on:
      - tdarr
    logging:
      driver: json-file
      options: *id002
  docs:
    <<: *network_tweaks
    image: nginx:alpine
    container_name: usenet-docs
    hostname: docs
    volumes:
      - ${STACK_ROOT:-.}/docs/.vitepress/dist:/usr/share/nginx/html:rw,z
    ports:
      - 4173:80
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 0
      resources:
        limits:
          cpus: '0.1'
          memory: 64M
        reservations:
          cpus: '0.05'
          memory: 32M
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --tries=1
        - --spider
        - http://localhost:80
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options: *id002
  stash:
    <<: *network_tweaks
    image: stashapp/stash:latest
    container_name: stash
    hostname: stash
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - STASH_STASH=/data/
      - STASH_GENERATED=/generated/
      - STASH_METADATA=/metadata/
      - STASH_CACHE=/cache/
      - STASH_PORT=9999
    volumes:
      - /etc/localtime:/etc/localtime:ro
      - ${CONFIG_ROOT:-/srv/usenet/config}/stash:/root/.stash:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/adult:/data:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}/stash/metadata:/metadata:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}/stash/cache:/cache:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}/stash/blobs:/blobs:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}/stash/generated:/generated:rw,z
      - ${MEDIA_ROOT:-/srv/usenet/media}/tv:/tv:rw,z
    ports:
      - 9999:9999
    devices:
      - /dev/dri:/dev/dri
    restart: unless-stopped
    logging:
      driver: json-file
      options: *id002
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
  komga:
    <<: *network_tweaks
    image: ghcr.io/gotson/komga:1.23.6
    container_name: komga
    user: 1000:1000
    security_opt:
      - label:disable
    logging: *id001
    environment:
      - JAVA_OPTS=-Xms4g -Xmx8g -Djava.io.tmpdir=/config/tmp
      - KOMGA_LIBRARIES_SCAN_CRON=0 0 * * * *
      # CORS: Allow Komf from all access points (localhost, 127.0.0.1, LAN IP)
      - KOMGA_CORS_ALLOWED_ORIGINS=http://localhost:8085,http://127.0.0.1:8085,http://192.168.6.167:8085
    volumes:
      - ${COMICS_ROOT:-/srv/usenet/books/Comics}:/comics:rw,z
      - ${COMICS_ROOT:-/srv/usenet/books/Comics}:/comics_mirror:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}/komga:/config:rw,z
    ports:
      - 8081:25600
    restart: unless-stopped
  komf:
    <<: *network_tweaks
    image: sndxr/komf:1.3.0
    container_name: komf
    logging: *id001
    environment:
      # Komga connection (container-to-container uses internal port 25600)
      - KOMGA_BASE_URI=http://komga:25600
      - KOMF_KOMGA_USER=${KOMF_KOMGA_USER:-j3lanzone@gmail.com}
      - KOMF_KOMGA_PASSWORD=${KOMF_KOMGA_PASSWORD:-fishing123}
      # Kavita connection
      - KOMF_KAVITA_BASE_URI=http://kavita:5000
      - KOMF_KAVITA_API_KEY=${KOMF_KAVITA_API_KEY:-2a0679fc-1ad7-414e-8ca1-e3f12d9164bf}
      # Metadata providers
      - KOMF_MAL_CLIENT_ID=${KOMF_MAL_CLIENT_ID:-7a9302bb7cc5f51d379ed81fea86a723}
      - KOMF_COMICVINE_API_KEY=${KOMF_COMICVINE_API_KEY:-174ed363d12baf12bc3b136a2f49e0a16cff5559}
      - KOMF_MANGAUPDATES_USERNAME=${KOMF_MANGAUPDATES_USERNAME:-}
      - KOMF_MANGAUPDATES_PASSWORD=${KOMF_MANGAUPDATES_PASSWORD:-}
      - KOMF_MANGADEX_CLIENT_ID=${KOMF_MANGADEX_CLIENT_ID:-}
      - KOMF_MANGADEX_CLIENT_SECRET=${KOMF_MANGADEX_CLIENT_SECRET:-}
      - KOMF_ANILIST_CLIENT_ID=${KOMF_ANILIST_CLIENT_ID:-}
      - KOMF_ANILIST_CLIENT_SECRET=${KOMF_ANILIST_CLIENT_SECRET:-}
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/komf:/config:rw,z
      - ${COMICS_ROOT:-/srv/usenet/books/Comics}:/comics:rw,z
    depends_on:
      - komga
      - kavita
    ports:
      - 8085:8085
    restart: unless-stopped
  kavita:
    <<: *network_tweaks
    image: jvmilazz0/kavita:latest
    container_name: kavita
    environment:
      - TZ=Etc/UTC
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/kavita:/kavita/config:rw,z
      - ${COMICS_ROOT:-/srv/usenet/books/Comics}:/comics:rw,z
      - ${EBOOKS_ROOT:-/srv/usenet/books/eBooks}:/books:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
    ports:
      - 5000:5000
    restart: unless-stopped
  suwayomi:
    <<: *network_tweaks
    image: ghcr.io/suwayomi/suwayomi-server:stable
    container_name: suwayomi
    environment:
      - TZ=Etc/UTC
    volumes:
      # Downloads go to staging folder, NOT directly into collection
      # A reorganizer script moves them to collection with proper naming
      - ${SUWAYOMI_DOWNLOADS:-/srv/usenet/downloads/suwayomi-chapters}:/home/suwayomi/.local/share/Tachidesk/downloads:rw,z
      - ${CONFIG_ROOT:-/srv/usenet/config}/suwayomi:/home/suwayomi/.local/share/Tachidesk:rw,z
    ports:
      - 4567:4567
    restart: unless-stopped
  audiobookshelf:
    <<: *network_tweaks
    image: ghcr.io/advplyr/audiobookshelf:latest
    container_name: audiobookshelf
    hostname: audiobookshelf
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - ${AUDIOBOOKSHELF_CONFIG:-/var/mnt/fast8tb/config/audiobookshelf}:/config:rw,z
      - ${AUDIOBOOKSHELF_CONFIG:-/var/mnt/fast8tb/config/audiobookshelf}/metadata:/metadata:rw,z
      - ${AUDIOBOOKS_ROOT:-/var/mnt/fast8tb/Cloud/OneDrive/Books/Audiobooks}:/audiobooks:rw,z
      - ${EBOOKS_ROOT:-/srv/usenet/books/eBooks}:/ebooks:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
    ports:
      - 13378:80
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test:
        - CMD
        - wget
        - --quiet
        - --tries=1
        - --spider
        - http://localhost:80/healthcheck
      interval: 30s
      timeout: 10s
      retries: 3
    logging:
      driver: json-file
      options: *id002
  aria2:
    <<: *network_tweaks
    build: ./aria2
    container_name: aria2
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - ARIA2_SECRET=${ARIA2_SECRET}
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/aria2:/config:rw,z
      - ${DOWNLOADS_ROOT:-/srv/usenet/downloads}:/downloads:rw,z
    ports:
      - 6800:6800
    restart: unless-stopped
  tautulli:
    <<: *network_tweaks
    image: lscr.io/linuxserver/tautulli:latest
    container_name: tautulli
    hostname: tautulli
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=America/Los_Angeles
    volumes:
      - ${CONFIG_ROOT:-/srv/usenet/config}/tautulli:/config:rw,z
    ports:
      - 8181:8181
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    logging:
      driver: json-file
      options: *id002
