# Complete Home Media Automation & Network Sharing Stack
# Version: 2.0 - Docker Swarm Ready with Integrated Samba/NFS
#
# This unified stack provides:
# - Complete *arr media automation suite
# - Integrated Samba (SMB/CIFS) file sharing
# - NFS exports for Linux/Unix clients  
# - Hot-swap storage support with JBOD architecture
# - Docker Swarm compatibility for multi-device deployment
# - Resource constraints and placement for different device types
#
# Services included:
# DOWNLOADERS:  SABnzbd (8080), Transmission (9092)
# AUTOMATION:   Sonarr (8989), Radarr (7878), Bazarr (6767), Prowlarr (9696)
# SPECIALIZED:  Whisparr (6969), Readarr (8787), Mylar3 (8090), YACReader (8082)
# INDEXERS:     Jackett (9117)
# MEDIA:        Jellyfin (8096)
# REQUESTS:     Overseerr (5055)
# TRANSCODING:  Tdarr (8265)
# OPTIMIZATION: Recyclarr (TRaSH Guide automation)
# SHARING:      Samba (139,445), NFS (2049)
# MONITORING:   Netdata (19999), Portainer (9000)
#
# JBOD Storage Architecture:
# Fast Storage (Primary): /media/joe/Fast_8TB_[1-3], /media/joe/Fast_4TB_[1-5]
# Archive Storage: /media/joe/Slow_4TB_[1-2], /media/joe/Slow_2TB_[1-2]
#
# Network Requirements:
# - Trusted subnet: 192.168.0.0/16 (adjust for your network)
# - Firewall: UFW rules for ports 139,445,2049,111 from trusted hosts
#
# Multi-Device Deployment Notes:
# - Use Docker Swarm for orchestration across multiple devices
# - Place storage-heavy services on devices with local storage
# - Place CPU-intensive services on more powerful devices
# - Use overlay networks for service communication
#
# Resource Classification:
# LOW:    Bazarr, Jackett, YacReader, Samba, NFS
# MEDIUM: Sonarr, Radarr, Prowlarr, Readarr, Mylar3, Whisparr
# HIGH:   SABnzbd, Transmission (during active downloads)

# Note: version field is deprecated in Compose v2.x and removed

# Define networks for service isolation and Swarm compatibility
networks:
  media_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
  
  sharing_network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.21.0.0/16

# Define volumes for configuration persistence across Swarm nodes
volumes:
  sabnzbd_config:
    driver: local
    driver_opts:
      type: bind
      o: bind
      device: /home/joe/usenet/config/sabnzbd
  
  transmission_config:
    driver: local
    driver_opts:
      type: bind
      o: bind
      device: /home/joe/usenet/config/transmission
  
  # Additional configs follow same pattern
  sonarr_config:
    driver: local
    driver_opts:
      type: bind
      o: bind
      device: /home/joe/usenet/config/sonarr
  radarr_config:
    driver: local
    driver_opts:
      type: bind
      o: bind
      device: /home/joe/usenet/config/radarr
  bazarr_config:
    driver: local
    driver_opts:
      type: bind
      o: bind
      device: /home/joe/usenet/config/bazarr
  prowlarr_config:
    driver: local
    driver_opts:
      type: bind
      o: bind
      device: /home/joe/usenet/config/prowlarr
  
  # Shared storage volumes
  downloads_volume:
    driver: local
    driver_opts:
      type: bind
      o: bind
      device: /home/joe/usenet/downloads
  
  media_fast_volume:
    driver: local
    driver_opts:
      type: bind
      o: bind
      device: /media/joe
  
  # Portainer persistent data
  portainer_data:
    driver: local

services:
  # ================================
  # SAMBA FILE SHARING SERVICE
  # Provides SMB/CIFS shares for Windows, macOS, Linux clients
  # Access: \\server-ip\ShareName
  # ================================
  samba:
    image: dperson/samba:latest
    container_name: samba
    hostname: media-server
    networks:
      - sharing_network
    ports:
      - "139:139"
      - "445:445"
    environment:
      - TZ=Etc/UTC
      - USERID=1000
      - GROUPID=1000
    command: >
      -u "joe;joe" 
      -s "Media;/media;yes;no;no;joe;joe;joe" 
      -s "Downloads;/downloads;yes;no;no;joe;joe;joe"
      -s "TV;/tv;yes;no;no;joe;joe;joe"
      -s "Movies;/movies;yes;no;no;joe;joe;joe"
      -s "Books;/books;yes;no;no;joe;joe;joe"
      -s "Comics;/comics;yes;no;no;joe;joe;joe"
      -s "Config;/config;yes;no;no;joe;joe;joe"
      -p
      -w "WORKGROUP"
      -n
    volumes:
      - /media/joe:/media:rw
      - downloads_volume:/downloads:rw
      - /home/joe/usenet/downloads:/downloads:rw
      - /home/joe/usenet/config:/config:rw
      # Mount all storage drives for flexible sharing
      - /media/joe/Fast_8TB_1:/tv/fast1:rw
      - /media/joe/Fast_8TB_2:/tv/fast2:rw
      - /media/joe/Fast_8TB_3:/tv/fast3:rw
      - /media/joe/Fast_4TB_1:/movies/fast1:rw
      - /media/joe/Fast_4TB_2:/movies/fast2:rw
      - /media/joe/Slow_4TB_1:/movies/archive1:rw
      - /media/joe/Slow_4TB_2:/movies/archive2:rw
      - /media/joe/Slow_2TB_1:/tv/archive1:rw
      - /media/joe/Slow_2TB_2:/tv/archive2:rw
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
          - node.labels.storage == true
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD", "smbclient", "-L", "localhost", "-U", "%"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ================================
  # NFS SERVER SERVICE  
  # Provides NFS exports for Linux/Unix clients
  # Mount: mount -t nfs server-ip:/media/joe /mount/point
  # ================================
  nfs-server:
    image: erichough/nfs-server:latest
    container_name: nfs-server
    hostname: nfs-server
    networks:
      - sharing_network
    ports:
      - "2049:2049"
      - "111:111"
      - "111:111/udp"
      - "32765:32765"
      - "32767:32767/udp"
    cap_add:
      - SYS_ADMIN
    privileged: true
    volumes:
      - /media/joe:/media/joe:rw
      - /home/joe/usenet/downloads:/downloads:rw
      - /home/joe/usenet/config:/config:rw
    environment:
      - SHARED_DIRECTORY=/media/joe
      - SHARED_DIRECTORY_2=/downloads
      - SHARED_DIRECTORY_3=/config
      - SYNC=true
      - PERMITTED=192.168.0.0/16
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
          - node.labels.storage == true
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.25'
          memory: 128M

  # ================================
  # SYSTEM MONITORING WITH NETDATA
  # Real-time system monitoring dashboard
  # Access at http://localhost:19999
  # ================================
  netdata:
    image: netdata/netdata:latest
    container_name: netdata
    hostname: netdata-media-server
    networks:
      - media_network
    ports:
      - "19999:19999"
    cap_add:
      - SYS_PTRACE
    security_opt:
      - apparmor:unconfined
    volumes:
      - /proc:/host/proc:ro
      - /sys:/host/sys:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - /etc/passwd:/host/etc/passwd:ro
      - /etc/group:/host/etc/group:ro
    environment:
      - DOCKER_HOST=/var/run/docker.sock
    restart: unless-stopped
    deploy:
      mode: global  # Run on every Swarm node
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M

  # ================================
  # PORTAINER SWARM MANAGEMENT
  # Docker Swarm management interface  
  # Access at http://localhost:9000
  # ================================
  portainer:
    image: portainer/portainer-ce:latest
    container_name: portainer
    networks:
      - media_network
    ports:
      - "9000:9000"
      - "8000:8000"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock
      - portainer_data:/data
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.role == manager
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ================================
  # SABnzbd - Usenet Downloader
  # Access at http://localhost:8080
  # Resource Intensive: Place on powerful nodes
  # ================================
  sabnzbd:
    image: lscr.io/linuxserver/sabnzbd:latest
    container_name: sabnzbd
    hostname: sabnzbd
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - sabnzbd_config:/config
      - downloads_volume:/downloads
    ports:
      - "8080:8080"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.performance == high
          - node.labels.storage == true
      resources:
        limits:
          cpus: '2.0'
          memory: 2G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8080"]
      interval: 30s
      timeout: 10s
      retries: 3

  # ================================
  # Transmission - BitTorrent Client
  # Access at http://localhost:9092
  # Resource Intensive: Place on powerful nodes
  # ================================
  transmission:
    image: lscr.io/linuxserver/transmission:latest
    container_name: transmission
    hostname: transmission
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - TRANSMISSION_WEB_HOME=/transmission-web-control/
    volumes:
      - transmission_config:/config
      - downloads_volume:/downloads
      - /home/joe/usenet/downloads/watch:/watch
    ports:
      - "9092:9091"
      - "51413:51413"
      - "51413:51413/udp"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.performance == high
          - node.labels.storage == true
      resources:
        limits:
          cpus: '1.5'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 256M

  # ================================
  # Sonarr - TV Show Management
  # Access at http://localhost:8989
  # Medium Resource Usage
  # ================================
  sonarr:
    image: lscr.io/linuxserver/sonarr:latest
    container_name: sonarr
    hostname: sonarr
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - sonarr_config:/config
      - downloads_volume:/downloads
      # Multi-drive TV storage mounts
      - /media/joe/Fast_8TB_1:/tv/fast1:rw
      - /media/joe/Fast_8TB_2:/tv/fast2:rw
      - /media/joe/Fast_8TB_3:/tv/fast3:rw
      - /media/joe/Fast_4TB_1:/tv/fast4:rw
      - /media/joe/Fast_4TB_2:/tv/fast5:rw
      - /media/joe/Slow_4TB_1:/tv/slow1:rw
      - /media/joe/Slow_4TB_2:/tv/slow2:rw
      - /media/joe/Slow_2TB_1:/tv/slow3:rw
      - /media/joe/Slow_2TB_2:/tv/slow4:rw
    ports:
      - "8989:8989"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      - sabnzbd
      - transmission

  # ================================
  # Radarr - Movie Management
  # Access at http://localhost:7878
  # Medium Resource Usage
  # ================================
  radarr:
    image: lscr.io/linuxserver/radarr:latest
    container_name: radarr
    hostname: radarr
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - radarr_config:/config
      - downloads_volume:/downloads
      # Multi-drive Movie storage mounts
      - /media/joe/Fast_8TB_1:/movies/fast1:rw
      - /media/joe/Fast_8TB_2:/movies/fast2:rw
      - /media/joe/Fast_8TB_3:/movies/fast3:rw
      - /media/joe/Fast_4TB_1:/movies/fast4:rw
      - /media/joe/Fast_4TB_2:/movies/fast5:rw
      - /media/joe/Slow_4TB_1:/movies/slow1:rw
      - /media/joe/Slow_4TB_2:/movies/slow2:rw
      - /media/joe/Slow_2TB_1:/movies/slow3:rw
      - /media/joe/Slow_2TB_2:/movies/slow4:rw
    ports:
      - "7878:7878"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 256M
    depends_on:
      - sabnzbd
      - transmission

  # ================================
  # Bazarr - Subtitle Management
  # Access at http://localhost:6767
  # Low Resource Usage - Can run anywhere
  # ================================
  bazarr:
    image: lscr.io/linuxserver/bazarr:latest
    container_name: bazarr
    hostname: bazarr
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - bazarr_config:/config
      # Same storage mounts as Sonarr/Radarr for subtitle access
      - /media/joe/Fast_8TB_1:/movies/fast1:rw
      - /media/joe/Fast_8TB_2:/movies/fast2:rw
      - /media/joe/Fast_8TB_3:/movies/fast3:rw
      - /media/joe/Fast_4TB_1:/movies/fast4:rw
      - /media/joe/Fast_4TB_2:/movies/fast5:rw
      - /media/joe/Fast_8TB_1:/tv/fast1:rw
      - /media/joe/Fast_8TB_2:/tv/fast2:rw
      - /media/joe/Fast_8TB_3:/tv/fast3:rw
      - /media/joe/Fast_4TB_1:/tv/fast4:rw
      - /media/joe/Fast_4TB_2:/tv/fast5:rw
      - /media/joe/Slow_4TB_1:/movies/slow1:rw
      - /media/joe/Slow_4TB_2:/movies/slow2:rw
      - /media/joe/Slow_2TB_1:/movies/slow3:rw
      - /media/joe/Slow_2TB_2:/movies/slow4:rw
      - /media/joe/Slow_4TB_1:/tv/slow1:rw
      - /media/joe/Slow_4TB_2:/tv/slow2:rw
      - /media/joe/Slow_2TB_1:/tv/slow3:rw
      - /media/joe/Slow_2TB_2:/tv/slow4:rw
    ports:
      - "6767:6767"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 128M
    depends_on:
      - sonarr
      - radarr

  # ================================
  # Prowlarr - Indexer Manager
  # Access at http://localhost:9696
  # Medium Resource Usage
  # ================================
  prowlarr:
    image: lscr.io/linuxserver/prowlarr:latest
    container_name: prowlarr
    hostname: prowlarr
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - prowlarr_config:/config
    ports:
      - "9696:9696"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.performance != low
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M

  # ================================
  # Whisparr - Adult Content Management
  # Access at http://localhost:6969
  # Medium Resource Usage
  # ================================
  whisparr:
    image: hotio/whisparr:nightly
    container_name: whisparr
    hostname: whisparr
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - /home/joe/usenet/config/whisparr:/config
      - downloads_volume:/downloads
      - /home/joe/usenet/media/movies:/movies
      - /home/joe/usenet/media/tv:/tv
    ports:
      - "6969:6969"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      placement:
        constraints:
          - node.labels.storage == true
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ================================
  # Readarr - Ebook Management
  # Access at http://localhost:8787
  # Low Resource Usage
  # ================================
  readarr:
    image: hotio/readarr:latest
    container_name: readarr
    hostname: readarr
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - /home/joe/usenet/config/readarr:/config
      - downloads_volume:/downloads
      - /home/joe/usenet/media/books:/books
    ports:
      - "8787:8787"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ================================
  # Mylar3 - Comic Book Management
  # Access at http://localhost:8090
  # Low Resource Usage
  # ================================
  mylar:
    image: linuxserver/mylar3:latest
    container_name: mylar
    hostname: mylar
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - /home/joe/usenet/config/mylar:/config
      - downloads_volume:/downloads
      - /home/joe/usenet/media/comics:/comics
    ports:
      - "8090:8090"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ================================
  # YacReader - Comic Reader & Library Server
  # Access at http://localhost:8082
  # Low Resource Usage
  # ================================
  yacreader:
    image: yacreader/yacreaderlibraryserver:latest
    container_name: yacreader
    hostname: yacreader
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - /home/joe/usenet/config/yacreader:/config
      - /home/joe/usenet/media/comics:/comics
    ports:
      - "8082:8080"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  # ================================
  # Jackett - Indexer Proxy (Fallback)
  # Access at http://localhost:9117
  # Low Resource Usage
  # ================================
  jackett:
    image: lscr.io/linuxserver/jackett:latest
    container_name: jackett
    hostname: jackett
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - /home/joe/usenet/config/jackett:/config
      - /home/joe/usenet/downloads/jackett:/downloads
    ports:
      - "9117:9117"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  # ================================
  # Jellyfin - Media Server
  # Access at http://localhost:8096
  # Medium Resource Usage
  # ================================
  jellyfin:
    image: lscr.io/linuxserver/jellyfin:latest
    container_name: jellyfin
    hostname: jellyfin
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - JELLYFIN_PublishedServerUrl=http://localhost:8096
    volumes:
      - /home/joe/usenet/config/jellyfin:/config
      - /home/joe/usenet/media:/media
      - /home/joe/usenet/media/movies:/movies
      - /home/joe/usenet/media/tv:/tv
      - /home/joe/usenet/media/music:/music
      - /home/joe/usenet/media/books:/books
    ports:
      - "8096:8096"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '1.0'
          memory: 1G

  # ================================
  # Overseerr - Request Management
  # Access at http://localhost:5055
  # Low Resource Usage
  # ================================
  overseerr:
    image: lscr.io/linuxserver/overseerr:latest
    container_name: overseerr
    hostname: overseerr
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - /home/joe/usenet/config/overseerr:/config
    ports:
      - "5055:5055"
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.5'
          memory: 512M

  # ================================
  # Recyclarr - TRaSH Guide Automation
  # Optimizes Sonarr/Radarr/Bazarr configurations
  # Low Resource Usage (Cron-based)
  # ================================
  recyclarr:
    image: ghcr.io/recyclarr/recyclarr:latest
    container_name: recyclarr
    hostname: recyclarr
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
    volumes:
      - /home/joe/usenet/config/recyclarr:/config
    restart: unless-stopped
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '0.25'
          memory: 256M

  # ================================
  # Tdarr - Automated Transcoding Engine
  # Access at http://localhost:8265
  # High Resource Usage (GPU/CPU intensive)
  # ================================
  tdarr:
    image: ghcr.io/haveagitgat/tdarr:latest
    container_name: tdarr
    hostname: tdarr
    networks:
      - media_network
    environment:
      - PUID=1000
      - PGID=1000
      - TZ=Etc/UTC
      - TDARR_INSTANCE_NAME=Main
      - serverIP=0.0.0.0
      - serverPort=8266
      - webUIPort=8265
      - internalNode=true
      - nodeID=MainNode
      # Hardware acceleration (uncomment for your GPU)
      # - NVIDIA_VISIBLE_DEVICES=all
      # - NVIDIA_DRIVER_CAPABILITIES=compute,utility,video
    volumes:
      - /home/joe/usenet/config/tdarr/server:/app/server
      - /home/joe/usenet/config/tdarr/configs:/app/configs
      - /home/joe/usenet/config/tdarr/logs:/app/logs
      - /home/joe/usenet/media:/media
      - /home/joe/usenet/media/movies:/movies
      - /home/joe/usenet/media/tv:/tv
      - /home/joe/usenet/downloads/tdarr_transcode:/temp
    ports:
      - "8265:8265"  # Web UI
      - "8266:8266"  # Server port
    restart: unless-stopped
    # Uncomment for GPU support
    # runtime: nvidia
    deploy:
      mode: replicated
      replicas: 1
      resources:
        limits:
          cpus: '4.0'      # Allow high CPU usage for transcoding
          memory: 4G       # Transcoding needs RAM for buffers
        reservations:
          cpus: '1.0'
          memory: 1G

